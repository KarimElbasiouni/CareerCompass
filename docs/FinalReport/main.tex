\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 85 Progress Report:\\CareerCompass}


\author{Karim Elbasiouni, Imran Chowdhury, Rami Abu Sultan \\
  \texttt{\{elbasik, chowdi13, abusultr\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

Job recruiters have an increased need for understanding how to better align job applicants skills and experience with the best available job posting. Manually reviewing unstructured resumes is slow, subjective, and is inefficient in a job market where thousands are applying for the same job opening. Our project reframes resume understanding as a resume-to-title classification task: ingest raw resumes, scrub PII, and map each document to a canonical job title and occupational family. The resulting labels can power dashboards that highlight dominant skill profiles and align job applicants with industry and company demands. Building on our proposal, we committed to fully automating data preparation and establishing a baseline model which will later be used for comparison to a more capable model.

Our approach for our baseline draws on both older and newer ideas in text classification. Earlier works such as \citet{Joachims1998SVM} showed that linear SVMs, when paired with TF-IDF features, could handle high-dimensional text very well. Around the same time, \citet{SaltonBuckley1988TermWeighting} explored how adding term weights could make document retrieval more effective by balancing how often a word appears in one document versus across many documents. Those findings directly influenced how we designed our TF-IDF + LinearSVC baseline, keeping it interpretable and fast enough to deploy for advising job recruiters.

In recent years, researchers have shifted toward models that capture meaning rather than just word frequency. A notable example is JobBERT introduced by \citet{Decorte2021JobBERT}, a transformer-based model designed to capture how job titles and skills are related. \citet{Liu2022Title2Vec} developed Title2Vec, which maps job titles into a shared numerical space so that similar roles can be grouped and compared more easily. Large pretrained models like BERT \citep{Devlin2019BERT} have proven effective at understanding general language and can be adapted to domains such as career analytics, where wording and phrasing vary across resumes.

\citet{inproceedings} compared TF-IDF with LinearSVC, LSTM models, and BERT on several text classification tasks. BERT reached higher accuracy on tasks that required context and phrasing awareness. The model improved with more training epochs and produced stronger results on difficult examples. TF-IDF with LinearSVC stayed fast, but its accuracy stopped improving early, while BERT continued to gain accuracy and handled subtle differences in language. This study supported our plan to add a BERT model to improve title prediction and reduce errors between similar roles.

We plan to fine tune BERT with methods that target the needs of our task. Researchers \citet{sun2020finetuneberttextclassification} have shown that fine tuning benefits from small design choices such as handling long text, selecting the right layers to update, and using careful learning rates. Further pre training on domain data can also strengthen the model when the target text differs from the original training corpus. Multi task fine tuning offers another path when related datasets are available. These strategies guide how we prepare BERT to work with resume text and job titles.

These studies helped guide how we approached CareerCompass. The early works gave us a solid foundation to begin our baseline model, and the newer methods discovered in recent years pointed us towards experimenting with fine-tuned BERT models to capture the subtle phrasing differences that can occur and better link resume text with standardized job titles.

\section{Dataset}

We use the publicly available Kaggle Resume Dataset by Rayyan Kauchali (2020; CC BY 4.0) \citep{Kauchali2020Resume} consisting of a mix of real and synthetic/anonymised resumes intended for NLP research. Each record includes structured text sections such as Summary, Skills, Experience, and Education, plus a noisy title-like category field (Category). Raw files are placed under \texttt{data/raw/}, and all preprocessing is orchestrated by \texttt{build\_clean\_dataset.py}.

Following our data plan, we applied a lightweight PII safeguard by redacting emails and phone numbers with deterministic regular expressions, then concatenated the key sections into a unified text field (\texttt{text\_clean}) and normalized it to \texttt{text\_norm}. Rows missing all major sections are dropped. This pipeline is reproducible via \texttt{build\_clean\_dataset.py} and supporting utilities (\texttt{src/data\_filter.py} and \texttt{src/text\_processing.py}), which write the canonical processed corpus to \texttt{data/processed/resumes\_v1.parquet} (and a CSV fallback).

Label construction follows our proposal and current implementation: raw titles from \texttt{Category} are normalized via a curated alias map into canonical titles (\texttt{title\_raw}), and each canonical title is mapped to a coarse occupation family (\texttt{y\_family}) using a lookup table (e.g., "Computers / IT", "Business / Finance"). This family mapping is conceptually aligned with SOC major groups, and unmapped or rare titles are assigned to \texttt{Other} to mitigate class imbalance.

Each record is assigned a stable \texttt{resume\_id} to support joins and splits. The processed dataset, exported in Parquet and CSV, serves as the single source for downstream feature extraction (TF-IDF) and supervised learning for two tasks: fine-grained job-title classification and coarse occupation family classification.

\section{Features \& Inputs}

Each resume was represented as a normalized text document derived by concatenating its major sections (Summary, Experience, Skills, and Education) after removing personal identifiers, converting all text to lowercase, and tokenizing the content. The resulting corpus was transformed into a bag-of-words representation using TF-IDF weighting, which encodes term importance by balancing local frequency against global rarity. Both unigrams and bigrams were included to capture short professional phrases while maintaining the independence assumptions of classical vector-space models. Tokens appearing in fewer than 3 resumes or in more than 90\% of documents were removed to reduce high-variance rare features and low-information common words, minimizing overfitting and improving generalization. The vectorizer was trained only on the training subset to avoid data leakage, and no further feature selection or augmentation was applied beyond the statistical weighting inherent in TF-IDF. This high-dimensional sparse matrix, about 80{,}000 features across 3{,}500 samples, serves as the fixed input for the baseline classifiers evaluated in the next section.

\section{Implementation}

\subsection{Data Preparation and Text Representation}

Resumes from the Kaggle dataset were first cleaned through a lightweight PII scrubbing procedure that deterministically redacts emails and phone numbers, followed by normalization of the major resume sections (Summary, Experience, Education, Skills). These sections were concatenated into a unified text field and filtered to remove records with insufficient content. Canonical job titles and coarse occupation families were created through alias normalization and family mapping.

For our classical baseline models, the normalized text was transformed into TF-IDF vectors using unigrams and bigrams with frequency thresholds that balance vocabulary size and noise reduction. This produced a high-dimensional sparse representation (\(\sim 80\text{K}\) features) that aligns with the classical ``bag-of-words'' modelling used in prior work on resume and document classification. TF-IDF was intentionally chosen as the first learned representation because it is easy to interpret, computationally inexpensive, and provides a strong baseline for text classification tasks.

Training, validation, and test splits followed an 80 / 10 / 10 stratified design to preserve label distributions and ensure reproducibility across all models.

\subsection{Baseline Models}

\paragraph{Majority baseline:}
Our simplest baseline predicts the most frequent job title in the training set. This establishes a trivial lower bound (5.7\% accuracy, macro-F1 near zero) and serves as a point of comparison for all subsequent models.

\paragraph{Linear SVM with TF-IDF:}
The next step was a linear SVM using the TF-IDF vectors. This model optimizes a soft-margin hinge-loss objective using LibLinear's coordinate-descent solver, which has historically performed well in high-dimensional sparse text settings. We tuned the regularization parameter \(C\) over a small grid and selected the best value using validation macro-F1, which is more appropriate than accuracy for imbalanced, multi-class classification.

The Linear SVM significantly outperformed the majority baseline, achieving 0.900 validation accuracy (macro-F1 0.936) and 0.886 test accuracy (macro-F1 0.924). These strong results confirm that TF-IDF remains a competitive baseline for well-structured text problems. The model's errors were mostly confined to semantically adjacent job titles (e.g., Python Developer vs. Data Science), motivating the exploration of contextual models.

\subsection{Transformer-Based Model (BERT)}

To move beyond lexical similarity and capture richer context, we implemented a transformer-based classifier using BERT. Resumes were tokenized with a maximum length of 512 tokens, and the pretrained \texttt{bert-base-uncased} encoder was fine-tuned using a softmax classification head. Fine-tuning was performed with AdamW, a learning rate of \(2\times10^{-5}\), weight decay of 0.01, and a small batch size due to GPU memory considerations. The training loop evaluated performance after each epoch and saved the best checkpoint based on macro-F1.

Although the model implementation is complete, full training on a local laptop was infeasible due to hardware and network limitations, particularly the inability to reliably download the BERT weights and the lack of GPU acceleration. For this reason, final fine-tuning is being executed on a collaborator's machine with a GPU, where the same splits and preprocessing pipeline are reused to maintain comparability with the SVM baseline. This aligns with the project expectations, which allow training to be performed on non-local hardware when justified.

\subsection{Evaluation Framework and Experiments}

A standardized evaluation suite was developed to ensure fair comparison across all models. This includes: accuracy and macro-F1 (primary metrics), top-\(k\) accuracy (\(k = 1, 3\)) for ranking performance, confusion matrices for qualitative inspection, and cross-model error analysis identifying hard examples and model-specific improvements or regressions.

These tools allowed us to examine not only which model performs best, but why. For example, error analysis revealed that SVM errors cluster around overlapping technical roles, suggesting that contextual representations like BERT may resolve such confusions.

We also conducted ablations on the SVM (varying \(C\)) and controlled TF-IDF vocabulary selection via frequency thresholds. The BERT model's hyperparameters (batch size, learning rate, and epochs) were also selected conservatively to prevent overfitting and to remain within computational limits.

\subsection{Implementation Rationale}

Our modelling pipeline reflects a deliberate, layered design:

\begin{enumerate}
    \item Majority vote baseline \(\rightarrow\) establishes minimum performance.
    \item Linear SVM + TF-IDF \(\rightarrow\) strong, interpretable classical model; serves as a tough baseline to beat.
    \item Fine-tuned BERT \(\rightarrow\) tests whether contextual semantic understanding yields better classification of nuanced job titles.
\end{enumerate}

Any discrepancy between expected and observed performance, especially with transformer models, can be directly tied to computational constraints rather than conceptual limitations, and these issues are discussed transparently in the Results and Error Analysis sections.

\section{Evaluation}

The evaluation followed the same standardized 80 / 10 / 10 train, validation, test split used at the progress stage, applied to the 3{,}500 resumes and 36 canonical job titles described earlier. Splits were generated with a fixed random seed (42) and stratification over the title label so that each class is represented in all splits. The training set contains all 36 classes with between 16 and 160 examples per class; validation and test each contain 350 examples, again covering all 36 classes with at least two examples per class. The most frequent titles (Java Developer, Data Science, Python Developer) have 160 training examples each, while the smallest classes (e.g., Technical Writer, Product Manager, Principal Engineer) have 16 to 24 training examples, leading to an imbalance of roughly 10:1 between the largest and smallest labels. This fixed, stratified split is used for all models (Majority, SVM, and BERT), rather than cross-validation, to simplify comparison and avoid the computational cost of repeatedly fine-tuning BERT.

All TF-IDF features (1 to 2 grams, min\_df = 3, max\_df = 0.9) are fit only on the training subset, and the fitted vectorizer is then applied to the validation and test sets, thereby preventing information leakage. Label encodings are regenerated consistently across runs using a shared label encoder so that every model sees the same mapping from title strings to integer IDs.

\subsection{Metrics}

Model performance is assessed with accuracy, macro-averaged F1, and Top-\(k\) accuracy (\(k \in \{1, 3\}\)), consistent with the framework used in the progress report. Accuracy captures overall correctness, while macro-F1 averages per-class F1 and therefore balances precision and recall across labels, countering the bias toward frequent titles. Top-\(k\) accuracy quantifies the proportion of resumes for which the correct title appears among the \(k\) highest-scoring predictions, which is particularly relevant in our setting where semantically close job titles (e.g., ``Java Developer'' vs ``Backend Developer'') may be interchangeable in context.

For both the SVM and BERT models, we also visualize confusion matrices on the validation and test sets. These plots show class-wise prediction counts and normalized row percentages, revealing which titles are consistently identified and which are systematically confused. Error-analysis CSVs complement these figures by listing individual resumes, their true labels, and model predictions, making it easy to inspect hard examples in more detail.

\subsection{Baseline Performance}

The majority-class baseline, which always predicts the most common training title, achieves only 0.057 accuracy and 0.003 macro-F1 on both validation and test splits. This confirms that naive strategies are essentially useless in this 36-way classification setting and provides a lower bound for comparison.

By contrast, the Linear SVC trained on TF-IDF features achieves 0.900 validation accuracy (macro-F1 0.936) and 0.886 test accuracy (macro-F1 0.924), with Top-3 accuracy exceeding 0.97 on both splits. These results reproduce and slightly refine the numbers reported in the progress report and indicate good generalization: the 1.4-point drop from validation to test is modest given the class imbalance. The SVM confusion matrices for validation and test (Figures~\ref{fig:confusion-val} and \ref{fig:confusion-test}) exhibit strong diagonal dominance, indicating high per-class accuracy, while small off-diagonal clusters highlight overlap between related titles such as Python Developer and Data Science or Java Developer and React Developer.

\begin{center}
\refstepcounter{figure}
\includegraphics[width=\columnwidth]{../../runs/svm_tfidf/confusion_val.png}\\
{\small Figure \thefigure: Confusion matrix on the validation set showing class-wise prediction counts and normalized row percentages for the Linear SVC baseline.}
\label{fig:confusion-val}
\end{center}

\begin{center}
\refstepcounter{figure}
\includegraphics[width=\columnwidth]{../../runs/svm_tfidf/confusion_test.png}\\
{\small Figure \thefigure: Confusion matrix on the test set illustrating consistent performance across splits and limited confusion between related job titles.}
\label{fig:confusion-test}
\end{center}

Per-class inspection confirms that smaller categories, including React Developer, Python Developer, SQL Developer, and Testing, have lower F1 scores (roughly 0.65 to 0.75) due to limited training data and lexical similarity across adjacent roles. These patterns motivated the introduction of a contextual model that can better exploit semantic cues in resumes.

\subsection{BERT Performance and Comparison}

We extended the evaluation to a BERT-based classifier fine-tuned on the same train/validation/test split. BERT uses the same title label space and assesses performance using identical metrics. On the validation set, the fine-tuned model attains 0.823 accuracy (macro-F1 0.888), which is lower than the SVM baseline. On the held-out test set, BERT reaches 0.880 accuracy and 0.924 macro-F1, effectively matching the SVM's macro-F1 but slightly underperforming it in accuracy (0.886 for SVM vs 0.880 for BERT).

The BERT confusion matrices for validation and test (Figures~\ref{fig:bert-confusion-val} and \ref{fig:bert-confusion-test}) show a similar qualitative pattern to those of the SVM: most mass lies on the diagonal, but off-diagonal blocks remain for closely related developer roles. This suggests that while BERT can capture more nuance in the text, the limited dataset size and class imbalance constrain its ability to significantly outperform the well-tuned linear baseline. In other words, our SVM baseline is strong enough that BERT does not offer a clear win across all classes, a result that we analyze further in the error-analysis section.

\begin{center}
\refstepcounter{figure}
\includegraphics[width=\columnwidth]{../../runs/bert_finetuned/confusion_val.png}\\
{\small Figure \thefigure: Confusion matrix on the validation set for the fine-tuned BERT model, showing similar patterns to the SVM baseline.}
\label{fig:bert-confusion-val}
\end{center}

\begin{center}
\refstepcounter{figure}
\includegraphics[width=\columnwidth]{../../runs/bert_finetuned/confusion_test.png}\\
{\small Figure \thefigure: Confusion matrix on the test set for the fine-tuned BERT model, demonstrating consistent performance with the validation set.}
\label{fig:bert-confusion-test}
\end{center}

\subsection{Top-\(k\) Behaviour and Hard Examples}

To better understand how useful the models are in a ranking or ``shortlisting'' scenario, we examined Top-\(k\) recall on the test set. For the SVM, recall@1 corresponds to standard accuracy (0.886), while recall@2, @3, @4, and @5 increase to 0.937, 0.974, 0.983, and 0.989, respectively (Figure~\ref{fig:topk-curve}). This means that for nearly 99\% of resumes the correct title appears within the top five SVM predictions, even when the top-1 prediction is incorrect. This is important for practical use: a recruiter viewing the top few suggested titles would almost always see the true label among them.

\begin{center}
\refstepcounter{figure}
\includegraphics[width=\columnwidth]{../../runs/evaluation/topk_curve.png}\\
{\small Figure \thefigure: Top-\(k\) recall curve comparing SVM and BERT on the test set, demonstrating the utility of ranking-based evaluation for practical deployment scenarios.}
\label{fig:topk-curve}
\end{center}

We complemented these aggregate curves with per-example error logs. On the test split, SVM is correct on 310 of 350 resumes (88.6\%), while the majority baseline is correct on only 20 (5.7\%). Approximately 10 to 11\% of resumes are misclassified by SVM, and around 10\% are ``hard cases'' where both SVM and the majority baseline fail. Inspection of these examples shows that they typically involve resumes whose skills or project descriptions are compatible with multiple adjacent roles (e.g., a Python-heavy resume labelled as Java Developer, or a mixed web-development profile labelled as Web Designing). These observations confirm that our chosen metrics (accuracy, macro-F1, Top-\(k\), and confusion matrices) are adequate and informative for understanding both global performance and class-specific weaknesses.

\section{Progress}

Our progress since the mid-semester report largely followed the trajectory we originally outlined, while incorporating several refinements motivated by TA feedback and practical constraints. In the progress report, we committed to (1) establishing a strong classical baseline using TF-IDF and a linear SVM, (2) developing an evaluation framework with clear quantitative and visual diagnostics, and (3) extending the system with a deeper contextual model such as BERT. All three goals were ultimately met, though the path to achieving the final objective evolved in response to implementation challenges.

We fully completed the classical pipeline as planned: the TF-IDF feature extractor, stratified dataset splits, and Linear SVC baseline were developed and validated early, and the resulting performance matched expectations from the progress stage. TA feedback emphasized the need for more varied and interpretable evaluation outputs, and we incorporated this directly by expanding our evaluation tooling to include confusion matrices for both splits, Top-\(k\) accuracy curves, and detailed error-case CSVs. These additions strengthened the analysis and ensured our evaluation strategy aligned with the expectations outlined during feedback.

The most significant change to our plan arose when implementing the transformer-based model. While our intention was to fine-tune BERT locally, we encountered repeated obstacles related to hardware limitations and model-weight downloads on the development machine. Fine-tuning BERT on CPU proved infeasible, and incomplete downloads prevented full local training. In response, we adapted by shifting BERT training to a collaborator's GPU-equipped environment. This adjustment preserved the scientific goals of our original plan while acknowledging real computational constraints. The resulting fine-tuned BERT model integrates cleanly with the rest of our pipeline, uses the same splits and metrics, and enabled a fair comparison against the SVM baseline.

Overall, the project stayed true to its intended roadmap, with the primary deviations driven by pragmatic concerns rather than conceptual changes. The expanded evaluation framework, incorporation of TA feedback, and successful completion of a transformer model (despite hardware barriers) reflect an iterative and responsive workflow that strengthened the final system.

\section{Error Analysis}

% \section*{Limitations}

\section*{Team Contributions}

Karim Elbasiouni implemented the end-to-end data pipeline, which includes configuration, raw JSONL loading, filtering and section validation, label normalization and family mapping, PII scrubbing, and the reproducible build script.
Imran Chowdhury implemented the modeling and evaluation, which includes TF-IDF feature construction and Linear SVC training pipeline, producing the final evaluation metrics and confusion matrices. Rami Abu Sultan compiled the progress report, integrated code results, and finalized the submission in \LaTeX{}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

\begin{filecontents}{custom.bib}
@misc{Kauchali2020Resume,
  author = {Rayyan Kauchali},
  title = {Resume Dataset},
  year = {2020},
  howpublished = {Kaggle},
  note = {CC BY 4.0},
  url = {https://www.kaggle.com/rkauchali/resume-dataset}
}
\end{filecontents}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
