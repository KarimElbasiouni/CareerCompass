\documentclass[12pt]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage{setspace}
\onehalfspacing
\usepackage{newtxtext,newtxmath}
\usepackage[dvipsnames]{xcolor} % added for link colors
\usepackage{array}
\usepackage{booktabs}
\usepackage{hyperref}            % keep after xcolor
\hypersetup{
  colorlinks=true,
  linkcolor=MidnightBlue,  % section / internal links
  urlcolor=RoyalPurple,    % URLs
  citecolor=BrickRed,      % (if you add citations later)
  filecolor=OliveGreen
}

\begin{document}

\begin{center}
  {\Large\bfseries Project Title: CareerCompass}\\[8pt]
  {\normalsize Group 85}
\end{center}

\section*{1. Team members}
\begin{itemize}
  \item Member 1: Imran Chowdhury - chowdi13@mcmaster.ca
  \item Member 2: Karim Elbasiouni — elbasik@mcmaster.ca
  \item Member 3: Rami Abu Sultan — abusultr@mcmaster.ca
\end{itemize}

\section*{2. Task title and overview}
\noindent\textbf{Title:} CareerCompass — Resume $\rightarrow$ Job Title \& Occupation Family Classification

\noindent\textbf{Overview:} Build a system that ingests a resume (PDF/DOCX/TXT or pasted text) and predicts (1) a fine-grained job title (e.g., Software Engineer, Data Analyst) and (2) a coarse occupation family (O*NET/SOC major group, e.g., Computer \& Mathematical Occupations).

\noindent\textbf{Significance:} Automated routing of candidates improves recruiter efficiency, speeds screening, and enables analytics on talent pools. 

\noindent\textbf{Challenges:} Resumes are unstructured; titles are noisy and at times synonymous (SWE vs Software Engineer II vs SDE); class imbalance; PII must be removed; hierarchical consistency between title and family must be maintained.

\setlength{\parindent}{0pt}

\section*{3. Task definition}
\subsection*{3.1 Data Type (what the model sees)}
The model operates primarily on de-identified resume text consolidated from the Summary, Experience, Skills, and Education sections after converting PDF or DOCX files to text. It supplements this corpus with derived auxiliary features such as section presence flags, token counts, simple skill-dictionary hits (e.g., Python, SQL, AWS), and basic length statistics while removing names, emails, phone numbers, addresses, and other obvious contact tokens before any storage or logging.

\medskip

\subsection*{3.2 Observed Label Space}
Fine-grained titles span 36 distinct categories that include roles such as Java Developer, Data Science, DevOps, Business Analyst, Product Manager, Technical Writer, SRE, and Engineering Manager. Counts are skewed, with several titles represented by 150--200 examples while roughly a dozen classes have fewer than 60, so alias normalization is required to keep the space tractable.

\medskip

Coarse occupation families align the titles to SOC/O*NET major groups with realistic coverage of five to seven families in this corpus, primarily Computer \& Mathematical along with Management, Business/Operations, Media/Communications, and a short tail. The full SOC major-group set remains available, but training and evaluation focus on families present in the data to avoid empty classes.

\medskip

\subsection*{3.3 Learning Setup}
The model architecture follows a two-head single-label classification design that leverages a shared encoder.
\begin{enumerate}
  \item Head A: Job Title operates as a single-label, multi-class classifier targeting approximately 25--30 cleaned classes after alias-normalizing near duplicates (e.g., SWE, SDE II, Senior Java Developer mapped to Software Engineer) and collapsing ultra-rare labels with fewer than 40--60 samples into an Other or parent bucket to manage imbalance and evaluation noise.
  \item Head B: Occupation Family provides single-label, multi-class predictions across the five to seven observed families, combining direct supervision on family labels with a consistency constraint enforced through the title-to-family mapping table.
\end{enumerate}

\medskip

\subsection*{3.4 Model I/O}
\begin{enumerate}
  \item \textit{Input} describes normalized resume text alongside the minimal derived features that capture structural cues.
  \item \textit{Output} returns title probabilities over $K\approx25$--$30$ classes (Top-1 and Top-3) and family probabilities over $F = 5$--$7$ families (Top-1 with $\sim$5--$7$ families) together with rationale highlights such as ``Python, REST, AWS, CI/CD''.
  \item \textit{Interfaces} support batch CSV processing and a REST \texttt{/predict} endpoint that produces \{\texttt{title\_top1}, \texttt{title\_top3}, \texttt{family}, \texttt{confidences}, \texttt{reasons[]}\}.
\end{enumerate}

\medskip

\subsection*{3.5 Why Two Heads (and not one label)}
Predicting both title and family improves robustness because the fine-grained title space is noisy while the family space is coarser and more stable, enables hierarchical checks by flagging disagreements between the predicted title and family, and strengthens intern or new-grad routing where the family often drives track selection while the title provides the display string.

\medskip

\subsection*{3.6 Evaluation Protocol}
\begin{enumerate}
  \item Title head metrics report Top-1 and Top-3 accuracy alongside macro-F1 to respect long-tail classes and confusion reviews among neighboring titles.
  \item Family head metrics track accuracy and hierarchical correctness to credit predictions that land in the right family when title classification misses.
  \item Calibration uses the Brier score and reliability curves to determine confidence thresholds for triage.
  \item Splits follow stratified train, validation, and test partitions with a held-out slice emphasizing resumes that look ``real-style'' rather than synthetic to assess generalization.
  \item Ablations compare text-only versus text-plus-auxiliary inputs, SVM or GBDT baselines versus DistilBERT or SBERT encoders, and configurations with or without tail collapse.
\end{enumerate}

\medskip

\subsection*{3.7 Planned Models}
The initial baseline relies on TF-IDF uni- and bi-gram features that feed Linear SVM and LightGBM classifiers for each head, providing a fast and dependable starting point while class weights or focal loss handle imbalance.

In the upgraded configuration, a DistilBERT or Sentence-BERT encoder shares representations across the two classifier heads and applies class weighting or focal loss to manage skew while improving accuracy and robustness.

\medskip

\section*{4. Problem, impact, and challenges}
\subsection*{4.1 Problem Statement}
The objective is to automatically map messy, free-form resumes to clear job titles and recruiting families.

\medskip

\subsection*{4.2 Real-World Impact}
First, high-volume funnels such as campus and new-grad intakes generate thousands of resumes within days, so a reliable classifier routes candidates to Software, Data/ML, Cloud/DevOps, or Product/Business Analyst tracks and trims manual triage time.

\medskip

Additionally, early-career resumes are short and stylistically uneven, so consistent predictions reduce reviewer fatigue and guard against missed signals such as projects or skills that could otherwise slip past subjective screens.

\medskip

Furthermore, token and skill-level rationales explain routing decisions—for example, ``Python, SQL, Power BI'' pointing to the Data Analyst family—so students receive faster feedback that guides coursework and project choices.

\medskip

Finally, family labels enable pipeline analytics that report on track health, skill coverage, and background diversity while keeping personally identifiable information scrubbed.

\medskip

\subsection*{4.3 Why It's Challenging (and How Our Design Addresses It)}
The task is challenging because resumes are unstructured, ambiguous, and imbalanced, requiring deliberate architectural and data-handling choices.
\begin{enumerate}
  \item Unstructured noisy text from multi-column PDFs and mixed formatting demands robust normalization, section-aware concatenation, and a transformer upgrade once the baselines stabilize.
  \item Synonymy and near duplicates such as SWE versus SDE or Backend with seniority suffixes require alias normalization, tail collapse, and Top-3 evaluation to capture realistic ambiguity.
  \item Long-tail class imbalance with roles represented by fewer than 60 examples calls for class weights or focal loss, the collapse strategy, and macro-F1 reporting.
  \item Hierarchy consistency between title and family depends on the two-head model backed by mapping checks and conflict flags that prompt human review.
  \item Privacy and fairness guardrails rely on PII scrubbing, exclusion of protected features, and slice metrics based on neutral proxies like resume length and section presence.
\end{enumerate}

\medskip

\subsection*{4.4 Justification}
Starting from 36 observed titles, the pipeline normalizes aliases, merges seniority variants, and targets $K\approx25$--$30$ classes so that granularity remains useful while approaching at least 60 examples per class whenever feasible. This collapse improves evaluations and supports defensible handling of sparse categories.

\medskip

The family space naturally populates five to seven SOC major groups, and training or evaluation is restricted to the families present in the dataset while keeping the mapping extensible for future data additions. This approach retains alignment with SOC definitions without forcing empty targets.

\medskip

Hierarchical agreement is enforced by mapping each predicted title to its family and comparing the result with the family head; any disagreements trigger a conflict flag and default routing to the family prediction for safer handling of intern and new-grad workflows.

\medskip

\subsection*{4.5 Risk and Mitigation}
Potential risks such as residual PII leaking through conversions, alias drift as new titles appear, label noise from weak supervision, class imbalance collapsing minority roles, overfitting to synthetic resumes, and opaque predictions are mitigated through automated scrubbing with spot audits, periodic alias-table reviews driven by active learning alerts, human-in-the-loop inspections of small cohorts each sprint, class-weighted or focal losses with tail buckets monitored via macro-F1, held-out ``real-style'' test slices with periodic shadow evaluation on fresh uploads, and reviewer-facing rationale logs that highlight key skills.

\setlength{\parindent}{15pt}

\section*{5. Data sources and collection plan}
\textbf{Primary Dataset:} We will use is the Kaggle Resume Dataset called \href{https://www.kaggle.com/datasets/rayyankauchali0/resume-dataset?resource=download}{Resume\_Dataset} by rayyankauchali0 (CC BY 4.0).
It contains a mixture of real, synthetic and LLM-generated anonymized resumes intended for NLP research and model training. It is inspired by public sources such as \href{https://huggingface.co/collections/ahmedheakl/resumeatlas-668047e86bc332049afd0b39}{ResumeAtlas} and
\href{https://huggingface.co/datasets/datasetmaster/resumes}{datasetmaster/resumes}, both which are on HuggingFace. The author also indicates that this dataset was preprocessed for anonymization and standardization.

\noindent \textbf{Provenance \& Citation}:  We will cite the Kaggle dataset, including the author, year, URL and License, and the inspirations in SOURCES.md. We will record the download date and the checksum in DATA\_CARD.md.

\noindent \textbf{What will be done with the data?}
The following outlines the procedures we will take:
\begin{enumerate}
  \item Download dataset and record the download date and the checksum
  \item Load data and perform filter out any rows that do not have the required fields (i.e. "Experience", "Education", "Summary", "Skills")
  \item Although the author says the data is already anonymized, we will run a PII safeguard and replace any names, emails, or phone numbers with a placeholder. We will record scrubbing procedure on DATA\_CARD.md so anyone can rerun it.
  \item We will construct the following labels:
  \begin{enumerate}
    \item \textbf{Title:} Use the current/most recent job title, and normalize any aliases (Example: SDE II becomes Software Engineer). Any rare titles will be collapsed into \textbf{Other}.
    \item \textbf{Family:} Map each normalized title its O*NET/SOC 2-digit major group (23 classes total) via a lookup table.
    \item \textbf{Manual Checks:} Manually check a representative sample of resumes to ensure that the automated labelling pipeline behaves as expected. Sample can be about 100 resumes.
  \end{enumerate}
  \item We will create train/validation/test splits grouped by role cluster and source type (real,synthetic, LLM-generated etc.). From the test dataset split, we create a test subset containing only real resumes. As of now,
  data split will be 80/10/10 respectively.
  \item We will not be scraping any data or calling any APIs. All of the data would be local according to our current understanding.
\end{enumerate}

\textbf{Note:} We plan on using the entiretly of the dataset (minus any filtered out data entries). All three splits (training, validation, test) will contain mixed data, and we will have a real-only
subset of the test dataset split as mentioned above. We will report results on both the mixed and real-only test sets and compare how our model performs.\\
\textbf{DATA\_CARD.md Metadata:}
\begin{itemize}
  \item Download date and Checksum
  \item Total number of entries and total number of entries per source type
  \item PII scrub procedure
  \item List of title classes with counts
  \item Link to the title -> family lookup table
\end{itemize}

\section*{6. Expected size of the dataset and 3 example data points with labels}

\noindent \textbf{Planned Dataset Size:}  
3,500+ resumes total; $\geq$3,200 usable after minor filtering (empty Experience, malformed records). Approximately 60 normalized title classes; 23 occupation families.

\medskip
\noindent \textbf{Three Example (Illustrative) Records:}

\begin{table}[ht]
\centering
\begin{tabular}{|p{2.5cm}|p{5.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Resume ID} & \textbf{Text Snippet} & \textbf{Title} & \textbf{Family} \\
\hline
REAL\_0001 & "...10 yrs software design \& development — senior Java developer / tech lead at Synnex (2014-Now)... REST APIs, Spring Boot..." & Software Engineer & 15 - Computer \& Mathematical \\
REAL\_0002 & "...enthusiastic Java developer (3 yrs) — reduced app memory 30\%, startup time 70\%. Roles at DaCoderz \& Quantexx..." & Java Developer → collapsed to Software Engineer & 15 - Computer \& Mathematical \\
REAL\_0003 & "...8 yrs delivering enterprise Java/J2EE solutions; Spring Boot, microservices, AWS; Sr Java Dev @ Fiserv (2021-Now)..." & Senior Java Developer → collapsed to Software Engineer & 15 - Computer \& Mathematical \\
\hline
\end{tabular}
\caption{Example Data points from the Dataset}
\label{tab:example-datapoints}
\end{table}


\section*{7. Proposed Solution}

\noindent \textbf{High-level Approach:}  
We plan to process resumes through text feature extraction, convert them into vector representations using TF-IDF, and use a \textbf{Linear SVM} as the baseline model for job title and occupation family classification. This baseline will help us establish performance metrics to benchmark against more advanced models.

\medskip
\noindent \textbf{Features / Inputs:}  
- \textbf{Text Features}:  
    - We will use \textbf{TF-IDF} to vectorize the resumes. This method captures important words in each resume, converting raw text data into numeric vectors suitable for machine learning models.
    - Additional features may include skill dictionary matches (e.g., Python, SQL, AWS), resume section presence (Summary, Experience, etc.), token counts, and average bullet length.
    
- \textbf{Target Labels}:  
    - \textbf{Job Title}: Fine-grained classification (e.g., Software Engineer, Data Scientist).
    - \textbf{Occupation Family}: Coarse classification (e.g., Computer \& Mathematical, Business Operations).

\medskip
\noindent \textbf{Baseline Model:}  
- \textbf{Linear SVM}: We will begin with a simple \textbf{Linear SVM} for both job title and occupation family classification. The model will be trained using TF-IDF vectors as features. This approach will establish baseline performance metrics (accuracy, F1 score) for evaluating future improvements.

\medskip
\noindent \textbf{Main Model:}  
- \textbf{DistilBERT Fine-Tuning}: After establishing baseline performance, we will fine-tune \textbf{DistilBERT} (a smaller, efficient version of BERT) to learn contextual relationships between words in resumes. This model will be evaluated using the same performance metrics, with the goal of outperforming the baseline \textbf{Linear SVM}.

\medskip
\noindent \textbf{Evaluation Plan:}  
- We will evaluate the models using \textbf{Top-1 accuracy} and \textbf{macro-F1 score} to handle class imbalance and ensure robust performance across all categories.
- We will also analyze the \textbf{confusion matrix} to identify where the model confuses similar job titles or occupation families.
  
\medskip
\noindent \textbf{Libraries / Tools:}  
- \textbf{scikit-learn} for TF-IDF vectorization and SVM implementation.
- \textbf{transformers} and \textbf{torch} for fine-tuning DistilBERT.
- \textbf{pandas} and \textbf{numpy} for data manipulation.

\medskip
\noindent \textbf{Existing Solutions / References:}  
- \textbf{BERT-based Models for Resume Classification}: Research shows BERT-based models excel at understanding context and improving classification performance on complex text tasks (e.g., [Devlin et al., 2019]).
- \textbf{Multi-class Text Classification}: Previous work has used models like SVM with TF-IDF for text classification tasks, including document categorization (e.g., [Joachims, 1998]).
- \textbf{Hierarchical Classification for Job Titles}: Several approaches explore hierarchical relationships in job title classification (e.g., [Vaswani et al., 2017]).



\end{document}
